{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy.spatial\n",
    "import functools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Kernel function generators\n",
    "def linear_kernel(X1, X2):\n",
    "    \"\"\"\n",
    "    Computes the linear kernel between two sets of vectors.\n",
    "    Args:\n",
    "        X1 - an n1xd matrix with vectors x1_1,...,x1_n1 in the rows\n",
    "        X2 - an n2xd matrix with vectors x2_1,...,x2_n2 in the rows\n",
    "    Returns:\n",
    "        matrix of size n1xn2, with x1_i^T x2_j in position i,j\n",
    "    \"\"\"\n",
    "    return np.dot(X1,np.transpose(X2))\n",
    " \n",
    "def RBF_kernel(X1,X2,sigma):\n",
    "    \"\"\"\n",
    "    Computes the RBF kernel between two sets of vectors   \n",
    "    Args:\n",
    "        X1 - an n1xd matrix with vectors x1_1,...,x1_n1 in the rows\n",
    "        X2 - an n2xd matrix with vectors x2_1,...,x2_n2 in the rows\n",
    "        sigma - the bandwidth (i.e. standard deviation) for the RBF/Gaussian kernel\n",
    "    Returns:\n",
    "        matrix of size n1xn2, with exp(-||x1_i-x2_j||^2/(2 sigma^2)) in position i,j\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "def polynomial_kernel(X1, X2, offset, degree):\n",
    "    \"\"\"\n",
    "    Computes the inhomogeneous polynomial kernel between two sets of vectors\n",
    "    Args:\n",
    "        X1 - an n1xd matrix with vectors x1_1,...,x1_n1 in the rows\n",
    "        X2 - an n2xd matrix with vectors x2_1,...,x2_n2 in the rows\n",
    "        offset, degree - two parameters for the kernel\n",
    "    Returns:\n",
    "        matrix of size n1xn2, with (offset + <x1_i,x2_j>)^degree in position i,j\n",
    "    \"\"\"\n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot kernel machine functions\n",
    "\n",
    "plot_step = .01\n",
    "xpts = np.arange(-5.0, 6, plot_step).reshape(-1,1)\n",
    "prototypes = np.array([-4,-1,0,2]).reshape(-1,1)\n",
    "\n",
    "# Linear kernel\n",
    "y = linear_kernel(prototypes, xpts) \n",
    "for i in range(len(prototypes)):\n",
    "    label = \"Linear@\"+str(prototypes[i,:])\n",
    "    plt.plot(xpts, y[i,:], label=label)\n",
    "plt.legend(loc = 'best')\n",
    "plt.show() \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Machine(object):\n",
    "    def __init__(self, kernel, prototype_points, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel(X1,X2) - a function return the cross-kernel matrix between rows of X1 and rows of X2 for kernel k\n",
    "            prototype_points - an Rxd matrix with rows mu_1,...,mu_R\n",
    "            weights - a vector of length R with entries w_1,...,w_R\n",
    "        \"\"\"\n",
    "\n",
    "        self.kernel = kernel\n",
    "        self.prototype_points = prototype_points\n",
    "        self.weights = weights\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Evaluates the kernel machine on the points given by the rows of X\n",
    "        Args:\n",
    "            X - an nxd matrix with inputs x_1,...,x_n in the rows\n",
    "        Returns:\n",
    "            Vector of kernel machine evaluations on the n points in X.  Specifically, jth entry of return vector is\n",
    "                Sum_{i=1}^R w_i k(x_j, mu_i)\n",
    "        \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train & test data; Convert to column vectors so it generalizes well to data in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train,data_test = np.loadtxt(\"krr-train.txt\"),np.loadtxt(\"krr-test.txt\")\n",
    "x_train, y_train = data_train[:,0].reshape(-1,1),data_train[:,1].reshape(-1,1)\n",
    "x_test, y_test = data_test[:,0].reshape(-1,1),data_test[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_ridge_regression(X, y, kernel, l2reg):\n",
    "    # TODO\n",
    "    return Kernel_Machine(kernel, X, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_step = .001\n",
    "xpts = np.arange(0 , 1, plot_step).reshape(-1,1)\n",
    "plt.plot(x_train,y_train,'o')\n",
    "l2reg = 0.0001\n",
    "for sigma in [.01,.1,1]:\n",
    "    k = functools.partial(RBF_kernel, sigma=sigma)\n",
    "    f = train_kernel_ridge_regression(x_train, y_train, k, l2reg=l2reg)\n",
    "    label = \"Sigma=\"+str(sigma)+\",L2Reg=\"+str(l2reg)\n",
    "    plt.plot(xpts, f.predict(xpts), label=label)\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(-1,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_step = .001\n",
    "xpts = np.arange(0 , 1, plot_step).reshape(-1,1)\n",
    "plt.plot(x_train,y_tiirain,'o')\n",
    "sigma= .02\n",
    "for l2reg in [.0001,.01,.1,2]:\n",
    "    k = functools.partial(RBF_kernel, sigma=sigma)\n",
    "    f = train_kernel_ridge_regression(x_train, y_train, k, l2reg=l2reg)\n",
    "    label = \"Sigma=\"+str(sigma)+\",L2Reg=\"+str(l2reg)\n",
    "    plt.plot(xpts, f.predict(xpts), label=label)\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(-1,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "\n",
    "class KernelRidgeRegression(BaseEstimator, RegressorMixin):  \n",
    "    \"\"\"sklearn wrapper for our kernel ridge regression\"\"\"\n",
    "     \n",
    "    def __init__(self, kernel=\"RBF\", sigma=1, degree=2, offset=1, l2reg=1):        \n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "        self.degree = degree\n",
    "        self.offset = offset\n",
    "        self.l2reg = l2reg \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This should fit classifier. All the \"work\" should be done here.\n",
    "        \"\"\"\n",
    "        if (self.kernel == \"linear\"):\n",
    "            self.k = linear_kernel\n",
    "        elif (self.kernel == \"RBF\"):\n",
    "            self.k = functools.partial(RBF_kernel, sigma=self.sigma)\n",
    "        elif (self.kernel == \"polynomial\"):\n",
    "            self.k = functools.partial(polynomial_kernel, offset=self.offset, degree=self.degree)\n",
    "        else:\n",
    "            raise ValueError('Unrecognized kernel type requested.')\n",
    "        \n",
    "        self.kernel_machine_ = train_kernel_ridge_regression(X, y, self.k, self.l2reg)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"kernel_machine_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "\n",
    "        return(self.kernel_machine_.predict(X))\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        # get the average square error\n",
    "        return(((self.predict(X)-y)**2).mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,PredefinedSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error,make_scorer\n",
    "import pandas as pd\n",
    "    \n",
    "test_fold = [-1]*len(x_train) + [0]*len(x_test)   #0 corresponds to test, -1 to train\n",
    "predefined_split = PredefinedSplit(test_fold=test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'kernel': ['RBF'],'sigma':[.1,1,10], 'l2reg': np.exp2(-np.arange(-5,5,1))},\n",
    "              {'kernel':['polynomial'],'offset':[-1,0,1], 'degree':[2,3,4],'l2reg':[10, .1, .01] },\n",
    "              {'kernel':['linear'],'l2reg': [10,1,.01]}]\n",
    "kernel_ridge_regression_estimator = KernelRidgeRegression()\n",
    "grid = GridSearchCV(kernel_ridge_regression_estimator, \n",
    "                    param_grid,\n",
    "                    cv = predefined_split,\n",
    "                    scoring = make_scorer(mean_squared_error,greater_is_better = False)\n",
    "                  # n_jobs = -1  #should allow parallelism, but crashes Python on my machine\n",
    "                   )\n",
    "grid.fit(np.vstack((x_train,x_test)),np.vstack((y_train,y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "# Flip sign of score back, because GridSearchCV likes to maximize,\n",
    "# so it flips the sign of the score if \"greater_is_better=FALSE\"\n",
    "df['mean_test_score'] = -df['mean_test_score']\n",
    "df['mean_train_score'] = -df['mean_train_score']\n",
    "cols_to_keep = [\"param_degree\", \"param_kernel\",\"param_l2reg\" ,\"param_offset\",\"param_sigma\",\n",
    "        \"mean_test_score\",\"mean_train_score\"]\n",
    "df_toshow = df[cols_to_keep].fillna('-')\n",
    "df_toshow.sort_values(by=[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the best polynomial and RBF fits you found\n",
    "plot_step = .01\n",
    "xpts = np.arange(-.5 , 1.5, plot_step).reshape(-1,1)\n",
    "plt.plot(x_train,y_train,'o')\n",
    "#Plot best polynomial fit\n",
    "offset= 1\n",
    "degree = 1\n",
    "l2reg = 1\n",
    "k = functools.partial(polynomial_kernel, offset=offset, degree=degree)\n",
    "f = train_kernel_ridge_regression(x_train, y_train, k, l2reg=l2reg)\n",
    "label = \"Offset=\"+str(offset)+\",Degree=\"+str(degree)+\",L2Reg=\"+str(l2reg)\n",
    "plt.plot(xpts, f.predict(xpts), label=label)\n",
    "#Plot best RBF fit\n",
    "sigma = 1\n",
    "l2reg= 1\n",
    "k = functools.partial(RBF_kernel, sigma=sigma)\n",
    "f = train_kernel_ridge_regression(x_train, y_train, k, l2reg=l2reg)\n",
    "label = \"Sigma=\"+str(sigma)+\",L2Reg=\"+str(l2reg)\n",
    "plt.plot(xpts, f.predict(xpts), label=label)\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(-1,1.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the SVM data\n",
    "#load the training and test sets\n",
    "data_train,data_test = np.loadtxt(\"svm-train.txt\"),np.loadtxt(\"svm-test.txt\")\n",
    "x_train, y_train = data_train[:,0:2], data_train[:,2].reshape(-1,1)\n",
    "x_test, y_test = data_test[:,0:2], data_test[:,2].reshape(-1,1)\n",
    "\n",
    "#determine predictions for the training set\n",
    "yplus = np.ma.masked_where(y_train[:,0]<=0, y_train[:,0])\n",
    "xplus = x_train[~np.array(yplus.mask)]\n",
    "yminus = np.ma.masked_where(y_train[:,0]>0, y_train[:,0])\n",
    "xminus = x_train[~np.array(yminus.mask)]\n",
    "\n",
    "#plot the predictions for the training set\n",
    "figsize = plt.figaspect(1)\n",
    "f, (ax) = plt.subplots(1, 1, figsize=figsize) \n",
    "\n",
    "pluses = ax.scatter (xplus[:,0], xplus[:,1], marker='+', c='r', label = '+1 labels for training set')\n",
    "minuses = ax.scatter (xminus[:,0], xminus[:,1], marker=r'$-$', c='b', label = '-1 labels for training set')\n",
    "\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=11)\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=11)\n",
    "ax.set_title('Training set size = %s'% len(data_train), fontsize=9)  \n",
    "ax.axis('tight')\n",
    "ax.legend(handles=[pluses, minuses], fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to help plot the decision regions\n",
    "# (Note: This code isn't necessarily entirely appropriate for the questions asked. So think about what you are doing.)\n",
    " \n",
    "sigma=1\n",
    "k = functools.partial(RBF_kernel, sigma=sigma)\n",
    "f = train_soft_svm(x_train, y_train, k, ...)\n",
    "\n",
    "#determine the decision regions for the predictions\n",
    "x1_min = min(x_test[:,0])\n",
    "x1_max= max(x_test[:,0])\n",
    "x2_min = min(x_test[:,1])\n",
    "x2_max= max(x_test[:,1])\n",
    "h=0.1\n",
    "xx, yy = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                     np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = f.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#determine the predictions for the test set\n",
    "y_bar = f.predict (x_test) \n",
    "yplus = np.ma.masked_where(y_bar<=0, y_bar)\n",
    "xplus = x_test[~np.array(yplus.mask)]\n",
    "yminus = np.ma.masked_where(y_bar>0, y_bar)\n",
    "xminus = x_test[~np.array(yminus.mask)]\n",
    "\n",
    "#plot the learned boundary and the predictions for the test set\n",
    "figsize = plt.figaspect(1)\n",
    "f, (ax) = plt.subplots(1, 1, figsize=figsize) \n",
    "decision =ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "pluses = ax.scatter (xplus[:,0], xplus[:,1], marker='+', c='b', label = '+1 prediction for test set')\n",
    "minuses = ax.scatter (xminus[:,0], xminus[:,1], marker=r'$-$', c='b', label = '-1 prediction for test set')\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=11)\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=11)\n",
    "ax.set_title('SVM with RBF Kernel: training set size = %s'% len(data_train), fontsize=9)  \n",
    "ax.axis('tight')\n",
    "ax.legend(handles=[pluses, minuses], fontsize=9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
