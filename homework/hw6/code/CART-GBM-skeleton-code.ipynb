{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = np.loadtxt('svm-train.txt')\n",
    "data_test = np.loadtxt('svm-test.txt')\n",
    "x_train, y_train = data_train[:, 0: 2], data_train[:, 2].reshape(-1, 1)\n",
    "x_test, y_test = data_test[:, 0: 2], data_test[:, 2].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change target to 0-1 label\n",
    "y_train_label = np.array(list(map(lambda x: 1 if x > 0 else 0, y_train))).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decision_Tree(BaseEstimator):\n",
    "     \n",
    "    def __init__(self, split_loss_function, leaf_value_estimator,\n",
    "                 depth=0, min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize the decision tree classifier\n",
    "\n",
    "        :param split_loss_function: method for splitting node\n",
    "        :param leaf_value_estimator: method for estimating leaf value\n",
    "        :param depth: depth indicator, default value is 0, representing root node\n",
    "        :param min_sample: an internal node can be splitted only if it contains points more than min_smaple\n",
    "        :param max_depth: restriction of tree depth.\n",
    "        '''\n",
    "        self.split_loss_function = split_loss_function\n",
    "        self.leaf_value_estimator = leaf_value_estimator\n",
    "        self.depth = depth\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        This should fit the tree classifier by setting the values self.is_leaf, \n",
    "        self.split_id (the index of the feature we want ot split on, if we're splitting),\n",
    "        self.split_value (the corresponding value of that feature where the split is),\n",
    "        and self.value, which is the prediction value if the tree is a leaf node.  If we are \n",
    "        splitting the node, we should also init self.left and self.right to be Decision_Tree\n",
    "        objects corresponding to the left and right subtrees. These subtrees should be fit on\n",
    "        the data that fall to the left and right,respectively, of self.split_value.\n",
    "        This is a recurisive tree building procedure. \n",
    "        \n",
    "        :param X: a numpy array of training data, shape = (n, m)\n",
    "        :param y: a numpy array of labels, shape = (n, 1)\n",
    "\n",
    "        :return self\n",
    "        '''\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        '''\n",
    "        Predict label by decision tree\n",
    "\n",
    "        :param instance: a numpy array with new data, shape (1, m)\n",
    "\n",
    "        :return whatever is returned by leaf_value_estimator for leaf containing instance\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        if instance[self.split_id] <= self.split_value:\n",
    "            return self.left.predict_instance(instance)\n",
    "        else:\n",
    "            return self.right.predict_instance(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_entropy(label_array):\n",
    "    '''\n",
    "    Calulate the entropy of given label list\n",
    "    \n",
    "    :param label_array: a numpy array of labels shape = (n, 1)\n",
    "    :return entropy: entropy value\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return entropy\n",
    "\n",
    "def compute_gini(label_array):\n",
    "    '''\n",
    "    Calulate the gini index of label list\n",
    "    \n",
    "    :param label_array: a numpy array of labels shape = (n, 1)\n",
    "    :return gini: gini index value\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common_label(y):\n",
    "    '''\n",
    "    Find most common label\n",
    "    '''\n",
    "    label_cnt = Counter(y.reshape(len(y)))\n",
    "    label = label_cnt.most_common(1)[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classification_Tree(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    loss_function_dict = {\n",
    "        'entropy': compute_entropy,\n",
    "        'gini': compute_gini\n",
    "    }\n",
    "\n",
    "    def __init__(self, loss_function='entropy', min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        :param loss_function(str): loss function for splitting internal node\n",
    "        '''\n",
    "\n",
    "        self.tree = Decision_Tree(self.loss_function_dict[loss_function],\n",
    "                                most_common_label,\n",
    "                                0, min_sample, max_depth)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tree.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        value = self.tree.predict_instance(instance)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training classifiers with different depth\n",
    "clf1 = Classification_Tree(max_depth=1)\n",
    "clf1.fit(x_train, y_train_label)\n",
    "\n",
    "clf2 = Classification_Tree(max_depth=2)\n",
    "clf2.fit(x_train, y_train_label)\n",
    "\n",
    "clf3 = Classification_Tree(max_depth=3)\n",
    "clf3.fit(x_train, y_train_label)\n",
    "\n",
    "clf4 = Classification_Tree(max_depth=4)\n",
    "clf4.fit(x_train, y_train_label)\n",
    "\n",
    "clf5 = Classification_Tree(max_depth=5)\n",
    "clf5.fit(x_train, y_train_label)\n",
    "\n",
    "clf6 = Classification_Tree(max_depth=6)\n",
    "clf6.fit(x_train, y_train_label)\n",
    "\n",
    "# Plotting decision regions\n",
    "x_min, x_max = x_train[:, 0].min() - 1, x_train[:, 0].max() + 1\n",
    "y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                        [clf1, clf2, clf3, clf4, clf5, clf6],\n",
    "                        ['Depth = {}'.format(n) for n in range(1, 7)]):\n",
    "\n",
    "    Z = np.array([clf.predict_instance(x) for x in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_train[:, 0], x_train[:, 1], c=y_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decision tree with tree model in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=5)\n",
    "clf.fit(x_train, y_train_label)\n",
    "export_graphviz(clf, out_file='tree_classifier.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize decision tree\n",
    "!dot -Tpng tree_classifier.dot -o tree_classifier.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='tree_classifier.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression Tree Specific Code\n",
    "def mean_absolute_deviation_around_median(y):\n",
    "    '''\n",
    "    Calulate the mean absolute deviation around the median of a given target list\n",
    "    \n",
    "    :param y: a numpy array of targets shape = (n, 1)\n",
    "    :return mae\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Regression_Tree():\n",
    "    '''\n",
    "    :attribute loss_function_dict: dictionary containing the loss functions used for splitting\n",
    "    :attribute estimator_dict: dictionary containing the estimation functions used in leaf nodes\n",
    "    '''\n",
    "\n",
    "    loss_function_dict = {\n",
    "        'mse': np.var,\n",
    "        'mae': mean_absolute_deviation_around_median\n",
    "    }\n",
    "\n",
    "    estimator_dict = {\n",
    "        'mean': np.mean,\n",
    "        'median': np.median\n",
    "    }\n",
    "    \n",
    "    def __init__(self, loss_function='mse', estimator='mean', min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize Regression_Tree\n",
    "        :param loss_function(str): loss function used for splitting internal nodes\n",
    "        :param estimator(str): value estimator of internal node\n",
    "        '''\n",
    "\n",
    "        self.tree = Decision_Tree(self.loss_function_dict[loss_function],\n",
    "                                  self.estimator_dict[estimator],\n",
    "                                  0, min_sample, max_depth)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tree.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict_instance(self, instance):\n",
    "        value = self.tree.predict_instance(instance)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit regression tree to one-dimensional regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_krr_train = np.loadtxt('krr-train.txt')\n",
    "data_krr_test = np.loadtxt('krr-test.txt')\n",
    "x_krr_train, y_krr_train = data_krr_train[:,0].reshape(-1,1),data_krr_train[:,1].reshape(-1,1)\n",
    "x_krr_test, y_krr_test = data_krr_test[:,0].reshape(-1,1),data_krr_test[:,1].reshape(-1,1)\n",
    "\n",
    "# Training regression trees with different depth\n",
    "clf1 = Regression_Tree(max_depth=1,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf1.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf2 = Regression_Tree(max_depth=2,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf2.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf3 = Regression_Tree(max_depth=3,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf3.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf4 = Regression_Tree(max_depth=4,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf4.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf5 = Regression_Tree(max_depth=5,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf5.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "clf6 = Regression_Tree(max_depth=6,  min_sample=1, loss_function='mae', estimator='median')\n",
    "clf6.fit(x_krr_train, y_krr_train)\n",
    "\n",
    "plot_size = 0.001\n",
    "x_range = np.arange(0., 1., plot_size).reshape(-1, 1)\n",
    "\n",
    "f2, axarr2 = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                        [clf1, clf2, clf3, clf4, clf5, clf6],\n",
    "                        ['Depth = {}'.format(n) for n in range(1, 7)]):\n",
    "\n",
    "    y_range_predict = np.array([clf.predict_instance(x) for x in x_range]).reshape(-1, 1)\n",
    "  \n",
    "    axarr2[idx[0], idx[1]].plot(x_range, y_range_predict, color='r')\n",
    "    axarr2[idx[0], idx[1]].scatter(x_krr_train, y_krr_train, alpha=0.8)\n",
    "    axarr2[idx[0], idx[1]].set_title(tt)\n",
    "    axarr2[idx[0], idx[1]].set_xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pseudo-residual function.\n",
    "#Here you can assume that we are using L2 loss\n",
    "\n",
    "def pseudo_residual_L2(train_target, train_predict):\n",
    "    '''\n",
    "    Compute the pseudo-residual based on current predicted value. \n",
    "    '''\n",
    "    return train_target - train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gradient_boosting():\n",
    "    '''\n",
    "    Gradient Boosting regressor class\n",
    "    :method fit: fitting model\n",
    "    '''\n",
    "    def __init__(self, n_estimator, pseudo_residual_func, learning_rate=0.1, min_sample=5, max_depth=3):\n",
    "        '''\n",
    "        Initialize gradient boosting class\n",
    "        \n",
    "        :param n_estimator: number of estimators (i.e. number of rounds of gradient boosting)\n",
    "        :pseudo_residual_func: function used for computing pseudo-residual\n",
    "        :param learning_rate: step size of gradient descent\n",
    "        '''\n",
    "        self.n_estimator = n_estimator\n",
    "        self.pseudo_residual_func = pseudo_residual_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''\n",
    "        Fit gradient boosting model\n",
    "        '''\n",
    "        # Your code goes here \n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        '''\n",
    "        Predict value\n",
    "        '''\n",
    "        # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-D GBM visualization - SVM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = x_train[:, 0].min() - 1, x_train[:, 0].max() + 1\n",
    "y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbt = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbt.fit(x_train, y_train)\n",
    "                   \n",
    "    Z = np.sign(gbt.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_train[:, 0], x_train[:, 1], c=y_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D GBM visualization - KRR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_size = 0.001\n",
    "x_range = np.arange(0., 1., plot_size).reshape(-1, 1)\n",
    "\n",
    "f2, axarr2 = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbm_1d = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbm_1d.fit(x_krr_train, y_krr_train)\n",
    "    \n",
    "    y_range_predict = gbm_1d.predict(x_range)\n",
    "\n",
    "    axarr2[idx[0], idx[1]].plot(x_range, y_range_predict, color='r')\n",
    "    axarr2[idx[0], idx[1]].scatter(x_krr_train, y_krr_train, alpha=0.8)\n",
    "    axarr2[idx[0], idx[1]].set_title(tt)\n",
    "    axarr2[idx[0], idx[1]].set_xlim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
